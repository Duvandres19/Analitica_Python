{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "mytext = \"Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\"\n",
    " \n",
    "print(sent_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\"\n",
    " \n",
    "print(sent_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Adam', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'Today', 'is', 'a', 'good', 'day', ',', 'see', 'you', 'dude', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\"\n",
    " \n",
    "print(word_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola señor adam ¿como esta?', 'Espero que todo vaya bien.', 'Hoy es un buen día, nos vemos amigo.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "mytext = \"Hola señor adam ¿como esta? Espero que todo vaya bien. Hoy es un buen día, nos vemos amigo.\"\n",
    " \n",
    "print(sent_tokenize(mytext,\"spanish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', 'señor', 'adam', '¿como', 'esta', '?', 'Espero', 'que', 'todo', 'vaya', 'bien', '.', 'Hoy', 'es', 'un', 'buen', 'día', ',', 'nos', 'vemos', 'amigo', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "mytext = \"Hola señor adam ¿como esta? Espero que todo vaya bien. Hoy es un buen día, nos vemos amigo.\"\n",
    " \n",
    "print(word_tokenize(mytext,\"spanish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clasificacion y etiquetado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a symptom of some physical hurt or disorder\n",
      "['the patient developed severe pain and distension']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "syn = wordnet.synsets(\"pain\")\n",
    " \n",
    "print(syn[0].definition())\n",
    " \n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the branch of information science that deals with natural language information\n",
      "large Old World boas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "syn = wordnet.synsets(\"NLP\")\n",
    " \n",
    "print(syn[0].definition())\n",
    " \n",
    "syn = wordnet.synsets(\"Python\")\n",
    " \n",
    "print(syn[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system', 'calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "synonyms = []\n",
    " \n",
    "for syn in wordnet.synsets('Computer'):\n",
    " \n",
    "    for lemma in syn.lemmas():\n",
    " \n",
    "        synonyms.append(lemma.name())\n",
    " \n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large', 'big', 'big']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    " \n",
    "antonyms = []\n",
    " \n",
    "for syn in wordnet.synsets(\"small\"):\n",
    " \n",
    "    for l in syn.lemmas():\n",
    " \n",
    "        if l.antonyms():\n",
    " \n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    " \n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stone\n",
      "speak\n",
      "bedroom\n",
      "joke\n",
      "lisa\n",
      "purpl\n",
      "----------------------\n",
      "stone\n",
      "speaking\n",
      "bedroom\n",
      "joke\n",
      "lisa\n",
      "purple\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(stemmer.stem('stones'))\n",
    " \n",
    "print(stemmer.stem('speaking'))\n",
    " \n",
    "print(stemmer.stem('bedroom'))\n",
    " \n",
    "print(stemmer.stem('jokes'))\n",
    " \n",
    "print(stemmer.stem('lisa'))\n",
    " \n",
    "print(stemmer.stem('purple'))\n",
    " \n",
    "print('----------------------')\n",
    " \n",
    "print(lemmatizer.lemmatize('stones'))\n",
    " \n",
    "print(lemmatizer.lemmatize('speaking'))\n",
    " \n",
    "print(lemmatizer.lemmatize('bedroom'))\n",
    " \n",
    "print(lemmatizer.lemmatize('jokes'))\n",
    " \n",
    "print(lemmatizer.lemmatize('lisa'))\n",
    " \n",
    "print(lemmatizer.lemmatize('purple'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
